
import os
import argparse
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import peft
from peft import get_peft_model, LoraConfig, TaskType

from mimir.dataset import PeptideDataset, create_dynamic_collate_fn
from mimir.tokenizer import AminoAcidTokenizer
from esm.models.esm3 import ESM3

def train(args):
    """
    Main training loop for Fine-Tuning ESM-3 with LoRA.
    
    Steps:
    1. Sets up the device (CUDA/CPU).
    2. Loads the tokenizer and dataset (from CSV).
    3. Initializes the ESM-3 model (Small Open version).
    4. Applies LoRA configuration for parameter-efficient fine-tuning.
    5. Runs the training loop, optimizing for likelihood of input sequences.
    
    Args:
        args: Parsed command-line arguments containing hyperparameters.
    """
    # 1. Setup Device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # 2. Prepare Data
    print("Loading tokenizer and dataset...")
    tokenizer = AminoAcidTokenizer()
    
    # Path to the dataset generated by scripts/generate_dataset.py
    dataset_path = os.path.join(os.path.dirname(__file__), "../data/dataset.csv")
    
    if not os.path.exists(dataset_path):
        print(f"Dataset not found at {dataset_path}. Please run generate_dataset.py first.")
        return

    # Initialize dataset
    dataset = PeptideDataset(
        dataset_path=dataset_path,
        tokenizer=tokenizer,
        max_length=args.max_length,
    )
    
    # Use dynamic collation to handle variable length batches efficiently
    collate_fn = create_dynamic_collate_fn(pad_idx=tokenizer.pad_idx)
    dataloader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        collate_fn=collate_fn
    )

    # 3. Load Model
    print("Loading ESM-3 model...")
    # 'esm3_sm_open_v1' is the small open-source weights for ESM-3
    model = ESM3.from_pretrained("esm3_sm_open_v1")
    model.to(device)
    
    # 4. Apply LoRA
    print("Applying LoRA...")
    # Target modules need to be verified. Common targets for transformers.
    # We'll print named modules if verbose.
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM, # ESM-3 is generative
        inference_mode=False,
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        target_modules=["q_proj", "v_proj"] # Guessing standard names. User should verify.
    )
    
    # Wrap model? 
    # ESM3 object might be a wrapper itself. 
    # Usually we wrap the underlying torch module.
    # Inspect model structure: model.model or similar?
    # If ESM3 inherits from nn.Module, we can wrap it.
    
    try:
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()
    except Exception as e:
        print(f"Failed to wrap with PEFT: {e}")
        print("Model modules:")
        for n, _ in model.named_modules():
            print(n)
        return

    # 5. Optimization
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)
    
    # 6. Train Loop
    model.train()
    for epoch in range(args.epochs):
        print(f"Epoch {epoch+1}/{args.epochs}")
        total_loss = 0
        
        for batch in tqdm(dataloader):
            tokens = batch["tokens"].to(device)
            # ESM-3 forward pass signature?
            # Usually: output = model(tokens)
            # Loss? ESM-3 might default to MLM or CLM?
            # Plan says "Maximize likelihood".
            
            optimizer.zero_grad()
            
            # ESM3 forward might return a dict.
            # We need to compute loss.
            # If using Causal LM, we shift labels.
            # But ESM-3 is masked diffusion or similar? 
            # Plan says: "Input: Masked tracks... Focus on Sequence".
            # "Prompt the fine-tuned model... and let it hallucinate".
            
            # For simplicity, we assume standard forward() returns loss or logits.
            # But with ESM-3 being complex, we might need a specific loss computation.
            # Let's try standard forward first.
            
            try:
                # ESM-3 input usually requires 'sequence_tokens'
                output = model(sequence_tokens=tokens)
                # Output likely has 'loss' if labels provided?
                # Or logits.
                
                # If we don't have labels, self-supervised?
                # Usually we pass labels=tokens for Causal/Masked.
                
                # Let's look at output keys if possible or assume standard.
                loss = output.loss if hasattr(output, 'loss') else None
                
                if loss is None:
                    # Compute loss manually? 
                    # This is tricky without API docs.
                    # We'll assume the model supports a simple forward for now.
                    # Or update this script after user feedback.
                    logits = output.logits
                    # Simple CrossEntropy
                    loss = torch.nn.functional.cross_entropy(
                        logits.view(-1, logits.size(-1)), 
                        tokens.view(-1),
                        ignore_index=tokenizer.pad_idx
                    )

                loss.backward()
                optimizer.step()
                total_loss += loss.item()
                
            except Exception as e:
                print(f"Error in training step: {e}")
                break
        
        print(f"Average Loss: {total_loss / len(dataloader)}")
        
        # Save checkpoint
        model.save_pretrained(f"checkpoints/epoch_{epoch}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, default=1)
    parser.add_argument("--batch_size", type=int, default=4)
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--max_length", type=int, default=32)
    parser.add_argument("--lora_r", type=int, default=8)
    parser.add_argument("--lora_alpha", type=int, default=32)
    parser.add_argument("--lora_dropout", type=float, default=0.1)
    args = parser.parse_args()
    
    train(args)
