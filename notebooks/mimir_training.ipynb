{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MÍMIR: Target-Conditioned Peptide Design\n",
        "\n",
        "Train the MÍMIR model on Google Colab using ESM-3 and LoRA for target-specific peptide generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "# Look for 'Tesla T4' (Free Tier, 16GB VRAM) or 'A100' (Pro, 40GB+ VRAM).\n",
        "# If you have a T4, you might need to reduce batch_size if you hit OOM.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Clone Repository & Install Dependencies\n",
        "We clone the MÍMIR repository and install the specific dependencies required for ESM-3 and LoRA fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eef7f790",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Repository URL\n",
        "REPO_URL = \"https://github.com/pmall/mimir.git\"\n",
        "\n",
        "!git clone $REPO_URL\n",
        "%cd mimir\n",
        "\n",
        "# Install dependencies from pyproject.toml\n",
        "# Dependencies for database (psycopg2) and env loading (python-dotenv) are removed as we use manual dataset upload.\n",
        "!pip install biopython>=1.86 esm>=3.2.1.post1 peft>=0.18.1 torch>=2.5.1 httpx>=0.28.1 huggingface_hub>=0.36.0 transformers>=4.48.1 accelerate>=1.12.0 bitsandbytes>=0.45.0 --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Authenticate with Hugging Face\n",
        "ESM-3 weights are gated. You must authenticate to download them. Ensure your token has access to `evolutionaryscale/esm3-sm-open-v1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!hf auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Setup & Verification\n",
        "We download the ESM-3 weights and perform a quick load test to ensure the environment is correctly configured before starting the heavy training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download ESM-3 weights\n",
        "!python scripts/download_weights.py\n",
        "\n",
        "# Verify successful load\n",
        "import esm\n",
        "from esm.models.esm3 import ESM3\n",
        "\n",
        "print(f\"ESM Version: {esm.__version__}\")\n",
        "try:\n",
        "    model = ESM3.from_pretrained(\"esm3_sm_open_v1\")\n",
        "    print(\"✅ ESM-3 model loaded successfully. Ready for training.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to load ESM-3: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Upload Dataset\n",
        "Upload your `dataset.csv` file containing the peptide-target pairs. This file is critical for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "print(\"Please upload your 'dataset.csv' file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    print(f'Received file \"{filename}\"')\n",
        "    target_path = 'data/dataset.csv'\n",
        "    shutil.move(filename, target_path)\n",
        "    print(f\"Moved {filename} to {target_path}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Fine-Tune ESM-3\n",
        "\n",
        "We now commence training. \n",
        "\n",
        "### Hyperparameters\n",
        "- **`epochs`**: **500**. Provides robust coverage for the combinatorial space.\n",
        "- **`batch_size`**: **256**. Since our peptides are short (<20 amino acids), we can use a large batch size even on T4 GPUs for efficient training.\n",
        "- **`lr`**: **1e-4**. Standard LoRA learning rate.\n",
        "- **`masking_boost_ratio`**: **0.5**. Boosts gradient for difficult, heavily masked samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python scripts/train.py --epochs 500 --batch_size 256 --lr 1e-4 --masking_boost_ratio 0.5"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
